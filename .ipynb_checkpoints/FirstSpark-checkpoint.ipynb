{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d54dd7d-248a-4fb2-a562-adb2f9a1681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloeff\n"
     ]
    }
   ],
   "source": [
    "print('Helloeff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc2b686-ea42-4ff3-89aa-fed50b99dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (2.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce98a6d-6351-4d1d-af97-a342bdb96199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\lenovo\\documents\\workspace\\sam-data\\spark-practs\\venv\\lib\\site-packages (from pyspark) (0.10.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920db7c0-0e47-4412-83f0-569f733495c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName2\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029bc3ed-d985-4848-b410-b2e332eabf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read data from a CSV file\n",
    "PATH = r'C:\\Users\\LENOVO\\Downloads\\sales.csv'\n",
    "df = spark.read.csv(PATH, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e062117f-50a3-4a48-a8fd-18ac5942415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+-----+----------+\n",
      "|  1|  A| 2023-01-01|India|    Swiggy|\n",
      "+---+---+-----------+-----+----------+\n",
      "|  2|  A| 2022-01-01|India|    Swiggy|\n",
      "|  2|  A| 2023-01-07|India|    Swiggy|\n",
      "|  3|  A| 2023-01-10|India|Restaurant|\n",
      "|  3|  A| 2022-01-11|India|    Swiggy|\n",
      "|  3|  A| 2023-01-11|India|Restaurant|\n",
      "|  2|  B| 2022-02-01|India|    Swiggy|\n",
      "|  2|  B| 2023-01-02|India|    Swiggy|\n",
      "|  1|  B| 2023-01-04|India|Restaurant|\n",
      "|  1|  B| 2023-02-11|India|    Swiggy|\n",
      "|  3|  B| 2023-01-16|India|    zomato|\n",
      "|  3|  B| 2022-02-01|India|    zomato|\n",
      "|  3|  C| 2023-01-01|India|    zomato|\n",
      "|  1|  C| 2023-01-01|   UK|    Swiggy|\n",
      "|  6|  C| 2022-01-07|   UK|    zomato|\n",
      "|  3|  D| 2023-02-16|   UK|Restaurant|\n",
      "|  5|  D| 2022-02-01|   UK|    zomato|\n",
      "|  3|  E| 2023-02-01|   UK|Restaurant|\n",
      "|  4|  E| 2023-02-01|   UK|    Swiggy|\n",
      "|  4|  E| 2023-02-07|   UK|Restaurant|\n",
      "|  2|  A| 2022-01-01|   UK|    Swiggy|\n",
      "+---+---+-----------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- A: string (nullable = true)\n",
      " |--  2023-01-01: string (nullable = true)\n",
      " |-- India: string (nullable = true)\n",
      " |-- Swiggy: string (nullable = true)\n",
      "\n",
      "+-------+------------------+----+-----------+-----+----------+\n",
      "|summary|                 1|   A| 2023-01-01|India|    Swiggy|\n",
      "+-------+------------------+----+-----------+-----+----------+\n",
      "|  count|               116| 116|        116|  116|       116|\n",
      "|   mean| 2.810344827586207|null|       null| null|      null|\n",
      "| stddev|1.2847858431149786|null|       null| null|      null|\n",
      "|    min|                 1|   A| 2022-01-01|India|Restaurant|\n",
      "|    max|                 6|   E| 2023-11-11|  USA|    zomato|\n",
      "+-------+------------------+----+-----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Perform basic statistics\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef177221-c57d-406b-b255-701bf15936f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11888\\1072231949.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>A</th>\n",
       "      <th>2023-01-01</th>\n",
       "      <th>India</th>\n",
       "      <th>Swiggy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>India</td>\n",
       "      <td>Swiggy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>India</td>\n",
       "      <td>Swiggy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>India</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>2022-01-11</td>\n",
       "      <td>India</td>\n",
       "      <td>Swiggy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>India</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>USA</td>\n",
       "      <td>zomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5</td>\n",
       "      <td>D</td>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>USA</td>\n",
       "      <td>zomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>USA</td>\n",
       "      <td>Swiggy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>2022-11-06</td>\n",
       "      <td>USA</td>\n",
       "      <td>zomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>2023-11-11</td>\n",
       "      <td>USA</td>\n",
       "      <td>zomato</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  A   2023-01-01  India      Swiggy\n",
       "0    2  A   2022-01-01  India      Swiggy\n",
       "1    2  A   2023-01-07  India      Swiggy\n",
       "2    3  A   2023-01-10  India  Restaurant\n",
       "3    3  A   2022-01-11  India      Swiggy\n",
       "4    3  A   2023-01-11  India  Restaurant\n",
       "..  .. ..          ...    ...         ...\n",
       "108  3  D   2022-11-16    USA      zomato\n",
       "109  5  D   2023-11-06    USA      zomato\n",
       "110  3  E   2023-11-06    USA      Swiggy\n",
       "111  4  E   2022-11-06    USA      zomato\n",
       "112  4  E   2023-11-11    USA      zomato\n",
       "\n",
       "[113 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "PATH = r'C:\\Users\\LENOVO\\Downloads\\sales.csv'\n",
    "pd.read_csv(PATH, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304135fc-b658-4687-9f34-73bf1d6e9abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TheHybridTech:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>YourAppName2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x206bdd9dc10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2020438e-3655-45ff-8ace-6dbb9c79d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('menu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b975ffe1-ceec-4bb5-ae21-5eab7d6a2c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2954b49b-983f-42fb-a919-fc06b27fc204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "|_c0|      _c1|  _c2|\n",
      "+---+---------+-----+\n",
      "| Id|     Name|Price|\n",
      "|  1|    PIZZA|  100|\n",
      "|  2|  Chowmin|  150|\n",
      "|  3| sandwich|  120|\n",
      "|  4|     Dosa|  110|\n",
      "|  5|  Biryani|   80|\n",
      "|  6|    Pasta|  180|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd58d02-1e2f-4bff-82e2-6731fa56f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| Id|     Name|Price|\n",
      "+---+---------+-----+\n",
      "|  1|    PIZZA|  100|\n",
      "|  2|  Chowmin|  150|\n",
      "|  3| sandwich|  120|\n",
      "|  4|     Dosa|  110|\n",
      "|  5|  Biryani|   80|\n",
      "|  6|    Pasta|  180|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('menu.csv')\n",
    "df_pyspark.show()\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e454ab1-1587-457e-b1f2-78bdcaec9b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d4ca13-cd78-432b-ab8d-486917791c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1f91f50-d58b-4942-aff8-8bd01e12d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| Id|     Name|Price|\n",
      "+---+---------+-----+\n",
      "|  1|    PIZZA|  100|\n",
      "|  2|  Chowmin|  150|\n",
      "|  3| sandwich|  120|\n",
      "|  4|     Dosa|  110|\n",
      "|  5|  Biryani|   80|\n",
      "|  6|    Pasta|  180|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('menu.csv',inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aa4c75c-3080-4d33-a270-188f94d5be3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'Name', 'Price']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e76b5ffb-23da-41b5-9478-4ab3201b84c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=1, Name=' PIZZA', Price=100), Row(Id=2, Name=' Chowmin', Price=150)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902a234d-7d0d-4569-9e13-7cf7d6e9a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    PIZZA|\n",
      "|  Chowmin|\n",
      "| sandwich|\n",
      "|     Dosa|\n",
      "|  Biryani|\n",
      "|    Pasta|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d5b90ca-9b24-4092-aea3-91ce7b274895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| Id|     Name|\n",
      "+---+---------+\n",
      "|  1|    PIZZA|\n",
      "|  2|  Chowmin|\n",
      "|  3| sandwich|\n",
      "|  4|     Dosa|\n",
      "|  5|  Biryani|\n",
      "|  6|    Pasta|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Id','Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db249758-c73c-4afe-aa35-3c37c1d5e32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Id', 'int'), ('Name', 'string'), ('Price', 'int')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b0cce33-f2ef-49be-9dac-3bb0ae3ee751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Id: string, Name: string, Price: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecd15f04-0f8a-413d-aa25-7ca9383eb6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+------------------+\n",
      "|summary|                Id|     Name|             Price|\n",
      "+-------+------------------+---------+------------------+\n",
      "|  count|                 6|        6|                 6|\n",
      "|   mean|               3.5|     null|123.33333333333333|\n",
      "| stddev|1.8708286933869707|     null| 36.14784456460256|\n",
      "|    min|                 1|  Biryani|                80|\n",
      "|    max|                 6| sandwich|               180|\n",
      "+-------+------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "842d9cb4-8cfd-4336-b40e-0babe7b86934",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding columns in data frame\n",
    "df_pyspark = df_pyspark.withColumn('Price in 2024',df_pyspark['Price']+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64490fb2-06a9-4f85-be43-89f5f0de7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+-------------+\n",
      "| Id|     Name|Price|Price in 2024|\n",
      "+---+---------+-----+-------------+\n",
      "|  1|    PIZZA|  100|          110|\n",
      "|  2|  Chowmin|  150|          160|\n",
      "|  3| sandwich|  120|          130|\n",
      "|  4|     Dosa|  110|          120|\n",
      "|  5|  Biryani|   80|           90|\n",
      "|  6|    Pasta|  180|          190|\n",
      "+---+---------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bc772f6-fce8-433e-a6e9-4e28a5d4b45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| Id|     Name|Price|\n",
      "+---+---------+-----+\n",
      "|  1|    PIZZA|  100|\n",
      "|  2|  Chowmin|  150|\n",
      "|  3| sandwich|  120|\n",
      "|  4|     Dosa|  110|\n",
      "|  5|  Biryani|   80|\n",
      "|  6|    Pasta|  180|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Drop the coolumns\n",
    "df_pyspark = df_pyspark.drop('Price in 2024')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1a633af-e24a-40bc-9397-f458a93b4084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| Id|Dish Name|Price|\n",
      "+---+---------+-----+\n",
      "|  1|    PIZZA|  100|\n",
      "|  2|  Chowmin|  150|\n",
      "|  3| sandwich|  120|\n",
      "|  4|     Dosa|  110|\n",
      "|  5|  Biryani|   80|\n",
      "|  6|    Pasta|  180|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rename the columns\n",
    "df_pyspark = df_pyspark.withColumnRenamed('Name','Dish Name')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7001036d-7b30-4044-8df5-c97523c5b3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark2 = spark.read.csv('test2.csv',header=True,inferSchema=True)\n",
    "df_spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1644cf80-490c-4ea7-a3b4-29b6d99264a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eba0214-7f6b-498a-972b-67cb91c48a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+----------------------+\n",
      "|     Name| age|Experience|Salary|Salary After Appraisal|\n",
      "+---------+----+----------+------+----------------------+\n",
      "|    Krish|  31|        10| 30000|                 32000|\n",
      "|Sudhanshu|  30|         8| 25000|                 27000|\n",
      "|    Sunny|  29|         4| 20000|                 22000|\n",
      "|     Paul|  24|         3| 20000|                 22000|\n",
      "|   Harsha|  21|         1| 15000|                 17000|\n",
      "|  Shubham|  23|         2| 18000|                 20000|\n",
      "|   Mahesh|null|      null| 40000|                 42000|\n",
      "|     null|  34|        10| 38000|                 40000|\n",
      "|     null|  36|      null|  null|                  null|\n",
      "+---------+----+----------+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2 = df_spark2.withColumn('Salary After Appraisal',df_spark2['Salary']+2000)\n",
    "df_spark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67314ae3-58be-4d25-b00d-e92ab533ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+----------------------+\n",
      "|Employee Name| age|Experience|Salary|Salary After Appraisal|\n",
      "+-------------+----+----------+------+----------------------+\n",
      "|        Krish|  31|        10| 30000|                 32000|\n",
      "|    Sudhanshu|  30|         8| 25000|                 27000|\n",
      "|        Sunny|  29|         4| 20000|                 22000|\n",
      "|         Paul|  24|         3| 20000|                 22000|\n",
      "|       Harsha|  21|         1| 15000|                 17000|\n",
      "|      Shubham|  23|         2| 18000|                 20000|\n",
      "|       Mahesh|null|      null| 40000|                 42000|\n",
      "|         null|  34|        10| 38000|                 40000|\n",
      "|         null|  36|      null|  null|                  null|\n",
      "+-------------+----+----------+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2 = df_spark2.withColumnRenamed('Name','Employee Name')\n",
    "df_spark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1af1666f-1bf1-4857-80d0-c07269d65af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|Employee Name| age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Krish|  31|        10| 30000|\n",
      "|    Sudhanshu|  30|         8| 25000|\n",
      "|        Sunny|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|       Harsha|  21|         1| 15000|\n",
      "|      Shubham|  23|         2| 18000|\n",
      "|       Mahesh|null|      null| 40000|\n",
      "|         null|  34|        10| 38000|\n",
      "|         null|  36|      null|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2 = df_spark2.drop('Salary After Appraisal')\n",
    "df_spark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bf9cc4f-a2d3-4431-9b02-66af889a3f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+\n",
      "|Employee Name|age|Experience|Salary|\n",
      "+-------------+---+----------+------+\n",
      "|        Krish| 31|        10| 30000|\n",
      "|    Sudhanshu| 30|         8| 25000|\n",
      "|        Sunny| 29|         4| 20000|\n",
      "|         Paul| 24|         3| 20000|\n",
      "|       Harsha| 21|         1| 15000|\n",
      "|      Shubham| 23|         2| 18000|\n",
      "+-------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2.na.drop().show() ### Drops all Rows even if single column in NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "503737e1-a0ae-4bc1-9271-8e7947022e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|Employee Name| age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Krish|  31|        10| 30000|\n",
      "|    Sudhanshu|  30|         8| 25000|\n",
      "|        Sunny|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|       Harsha|  21|         1| 15000|\n",
      "|      Shubham|  23|         2| 18000|\n",
      "|       Mahesh|null|      null| 40000|\n",
      "|         null|  34|        10| 38000|\n",
      "|         null|  36|      null|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### any==how any means drop rows if any of the column is null and how=all means if all columns in that record is null\n",
    "df_spark2.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68498785-0851-4c0c-a8e8-d92555d176a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|Employee Name| age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Krish|  31|        10| 30000|\n",
      "|    Sudhanshu|  30|         8| 25000|\n",
      "|        Sunny|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|       Harsha|  21|         1| 15000|\n",
      "|      Shubham|  23|         2| 18000|\n",
      "|       Mahesh|null|      null| 40000|\n",
      "|         null|  34|        10| 38000|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##threshold -> limit or count of columns which can stay null upto which it will be dropped\n",
    "df_spark2.na.drop(how='any',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e58332c2-2062-4d0e-96f7-82e682857bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+\n",
      "|Employee Name|age|Experience|Salary|\n",
      "+-------------+---+----------+------+\n",
      "|        Krish| 31|        10| 30000|\n",
      "|    Sudhanshu| 30|         8| 25000|\n",
      "|        Sunny| 29|         4| 20000|\n",
      "|         Paul| 24|         3| 20000|\n",
      "|       Harsha| 21|         1| 15000|\n",
      "|      Shubham| 23|         2| 18000|\n",
      "|         null| 34|        10| 38000|\n",
      "+-------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Subset dropping record based on column name being Null\n",
    "df_spark2.na.drop(how='any',subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f35f920-31d5-42af-898b-389047512370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|Employee Name| age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Krish|  31|        10| 30000|\n",
      "|    Sudhanshu|  30|         8| 25000|\n",
      "|        Sunny|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|       Harsha|  21|         1| 15000|\n",
      "|      Shubham|  23|         2| 18000|\n",
      "|       Mahesh|null|      null| 40000|\n",
      "|         null|  34|        10| 38000|\n",
      "|         null|  36|      null|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Filling missing values\n",
    "df_spark2.na.fill('Missing Values',['Experience','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6936eebb-d35f-4903-91f0-185fbe2d8a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after filling null values:\n",
      "+-------------+----+----------+------+\n",
      "|Employee Name| age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Krish|  31|        10| 30000|\n",
      "|    Sudhanshu|  30|         8| 25000|\n",
      "|        Sunny|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|       Harsha|  21|         1| 15000|\n",
      "|      Shubham|  23|         2| 18000|\n",
      "|       Mahesh|null|      null| 40000|\n",
      "|         null|  34|        10| 38000|\n",
      "|         null|  36|      null|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filled_df_spark2 = df_spark2.na.fill('Missing Values', ['Experience', 'age'])\n",
    "\n",
    "# Show the DataFrame after filling null values\n",
    "print(\"DataFrame after filling null values:\")\n",
    "filled_df_spark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "610ed5ed-8f4b-4e7b-9ae1-e220feb3d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputer function\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=['age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age','Experience','Salary']]\n",
    ").setStrategy(\"means\")\n",
    "# Add imputation cols to df\n",
    "# imputer.fit(df_spark2).transform(df_spark2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087938d7-62b9-453f-8740-6a38b50818d0",
   "metadata": {},
   "source": [
    "Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fca5c59-58fb-46ea-9e0f-c515a96d7078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark2 = spark.read.csv('test2.csv',header=True,inferSchema=True)\n",
    "df_spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01c92e47-90a3-4b75-a59e-d19e6119221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Salary of people less than or equal to 2000\n",
    "df_spark2.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23371fa0-f1db-4d9b-8acb-15ffd78a4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|age|\n",
      "+-------+---+\n",
      "|  Sunny| 29|\n",
      "|   Paul| 24|\n",
      "| Harsha| 21|\n",
      "|Shubham| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2.filter(\"Salary<=20000\").select(['Name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44a3bfba-4803-48eb-a6b6-ee17ec0babc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2.filter((df_spark2['Salary']<=20000) | (df_spark2['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26c644ef-d433-4747-a0d4-cf3ce6b5c4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Not operation\n",
    "df_spark2.filter(~(df_spark2['Salary']<=20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f02dc-e836-46ce-911e-0cc6b9c2a6d2",
   "metadata": {},
   "source": [
    "GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f0930a0-d6d8-4499-823c-263d95de7f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Departments: string, salary: int]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark2 = spark.read.csv('test3.csv',header=True,inferSchema=True)\n",
    "df_spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68830812-1156-458b-a9f2-086ad561b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
